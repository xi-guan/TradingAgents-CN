"""
æ¨ç†è¿‡ç¨‹å¤„ç†å™¨

ä¸“é—¨å¤„ç†æ”¯æŒæ¨ç†çš„æ¨¡å‹ï¼š
- OpenAI o1 (o1-preview, o1-mini)
- DeepSeek R1 (deepseek-r1)
- Claude Extended Thinking (claude-3-opus with extended_thinking)
"""

from typing import Dict, Any, List, Optional
from datetime import datetime
from dataclasses import dataclass
from enum import Enum

from tradingagents.utils.logging_manager import get_logger

logger = get_logger('middleware.reasoning_handler')


class ReasoningModelType(Enum):
    """æ¨ç†æ¨¡å‹ç±»å‹"""
    OPENAI_O1 = "openai_o1"              # OpenAI o1-preview, o1-mini
    DEEPSEEK_R1 = "deepseek_r1"          # DeepSeek R1
    CLAUDE_THINKING = "claude_thinking"   # Claude Extended Thinking
    UNKNOWN = "unknown"


@dataclass
class ReasoningTrace:
    """æ¨ç†è½¨è¿¹æ•°æ®ç»“æ„"""
    model_type: ReasoningModelType
    reasoning_content: str
    reasoning_tokens: int
    timestamp: datetime
    metadata: Dict[str, Any]

    def get_summary(self, max_length: int = 200) -> str:
        """è·å–æ¨ç†æ‘˜è¦"""
        if len(self.reasoning_content) <= max_length:
            return self.reasoning_content

        return self.reasoning_content[:max_length] + "..."

    def get_word_count(self) -> int:
        """è·å–æ¨ç†å­—æ•°"""
        # ç®€å•ç»Ÿè®¡ï¼šä¸­æ–‡æŒ‰å­—æ•°ï¼Œè‹±æ–‡æŒ‰å•è¯æ•°
        chinese_chars = sum(1 for c in self.reasoning_content if '\u4e00' <= c <= '\u9fff')
        english_words = len([w for w in self.reasoning_content.split() if w.isalpha()])

        return chinese_chars + english_words


class ReasoningHandler:
    """
    æ¨ç†è¿‡ç¨‹å¤„ç†å™¨

    åŠŸèƒ½ï¼š
    1. è¯†åˆ«æ¨ç†æ¨¡å‹ç±»å‹
    2. æå–å’Œè§£ææ¨ç†å†…å®¹
    3. æ ¼å¼åŒ–æ¨ç†è¿‡ç¨‹å±•ç¤º
    4. ç»Ÿè®¡æ¨ç† token æ¶ˆè€—
    5. åˆ†ææ¨ç†è´¨é‡æŒ‡æ ‡
    """

    def __init__(
        self,
        enable_detailed_logging: bool = True,
        reasoning_max_display: int = 1000
    ):
        """
        åˆå§‹åŒ–æ¨ç†å¤„ç†å™¨

        Args:
            enable_detailed_logging: æ˜¯å¦è¯¦ç»†è®°å½•æ¨ç†è¿‡ç¨‹
            reasoning_max_display: æ¨ç†è¿‡ç¨‹æœ€å¤§æ˜¾ç¤ºé•¿åº¦
        """
        self.enable_detailed_logging = enable_detailed_logging
        self.reasoning_max_display = reasoning_max_display

        self.reasoning_traces: List[ReasoningTrace] = []
        self.total_reasoning_tokens = 0

    def detect_model_type(self, message: Any, response_metadata: Dict = None) -> ReasoningModelType:
        """
        æ£€æµ‹æ¨ç†æ¨¡å‹ç±»å‹

        Args:
            message: AI æ¶ˆæ¯
            response_metadata: å“åº”å…ƒæ•°æ®

        Returns:
            æ¨ç†æ¨¡å‹ç±»å‹
        """
        if response_metadata is None:
            response_metadata = getattr(message, 'response_metadata', {})

        # æ£€æŸ¥æ¨¡å‹åç§°
        model_name = response_metadata.get('model_name', '').lower()

        if 'o1' in model_name:
            return ReasoningModelType.OPENAI_O1
        elif 'deepseek-r1' in model_name or 'deepseek_r1' in model_name:
            return ReasoningModelType.DEEPSEEK_R1
        elif 'claude' in model_name and 'thinking' in str(response_metadata):
            return ReasoningModelType.CLAUDE_THINKING

        # æ£€æŸ¥ provider
        provider = response_metadata.get('model_provider', '').lower()

        if provider == 'openai' and 'reasoning' in response_metadata:
            return ReasoningModelType.OPENAI_O1
        elif provider == 'deepseek':
            return ReasoningModelType.DEEPSEEK_R1
        elif provider == 'anthropic' and 'thinking' in response_metadata:
            return ReasoningModelType.CLAUDE_THINKING

        return ReasoningModelType.UNKNOWN

    def extract_reasoning(
        self,
        message: Any,
        model_type: Optional[ReasoningModelType] = None
    ) -> Optional[ReasoningTrace]:
        """
        æå–æ¨ç†å†…å®¹

        Args:
            message: AI æ¶ˆæ¯
            model_type: æ¨¡å‹ç±»å‹ï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨æ£€æµ‹ï¼‰

        Returns:
            æ¨ç†è½¨è¿¹å¯¹è±¡ï¼Œå¦‚æœæ²¡æœ‰æ¨ç†å†…å®¹åˆ™è¿”å› None
        """
        if model_type is None:
            model_type = self.detect_model_type(message)

        response_metadata = getattr(message, 'response_metadata', {})

        # æ ¹æ®æ¨¡å‹ç±»å‹æå–æ¨ç†å†…å®¹
        reasoning_content = None
        reasoning_tokens = 0
        metadata = {}

        if model_type == ReasoningModelType.OPENAI_O1:
            # OpenAI o1 æ¨ç†åœ¨ response_metadata ä¸­
            reasoning_content = response_metadata.get('reasoning')
            reasoning_tokens = response_metadata.get('reasoning_tokens', 0)

            # è·å–é¢å¤–ä¿¡æ¯
            metadata = {
                'completion_tokens': response_metadata.get('completion_tokens', 0),
                'reasoning_tokens': reasoning_tokens,
                'model': response_metadata.get('model_name', 'o1-preview')
            }

        elif model_type == ReasoningModelType.DEEPSEEK_R1:
            # DeepSeek R1 æ¨ç†åœ¨ content_blocks æˆ– response_metadata ä¸­
            if hasattr(message, 'content_blocks'):
                for block in message.content_blocks:
                    if isinstance(block, dict) and block.get('type') == 'reasoning':
                        reasoning_content = block.get('reasoning')
                        break
                    elif hasattr(block, 'type') and block.type == 'reasoning':
                        reasoning_content = block.reasoning
                        break

            if not reasoning_content:
                reasoning_content = response_metadata.get('reasoning')

            # ä¼°ç®— tokensï¼ˆDeepSeek å¯èƒ½ä¸æä¾›ï¼‰
            if reasoning_content:
                reasoning_tokens = int(len(reasoning_content) * 1.5)

            metadata = {
                'model': response_metadata.get('model_name', 'deepseek-r1'),
                'estimated_tokens': True
            }

        elif model_type == ReasoningModelType.CLAUDE_THINKING:
            # Claude Extended Thinking
            if hasattr(message, 'content_blocks'):
                for block in message.content_blocks:
                    if isinstance(block, dict) and block.get('type') == 'thinking':
                        reasoning_content = block.get('thinking')
                        break
                    elif hasattr(block, 'type') and block.type == 'thinking':
                        reasoning_content = block.thinking
                        break

            if not reasoning_content:
                reasoning_content = response_metadata.get('thinking')

            # Claude æä¾› thinking tokens
            reasoning_tokens = response_metadata.get('usage', {}).get('input_tokens_cache_create', 0)

            metadata = {
                'model': response_metadata.get('model_name', 'claude-3-opus'),
                'thinking_signature': response_metadata.get('thinking_signature')
            }

        if not reasoning_content:
            return None

        # åˆ›å»ºæ¨ç†è½¨è¿¹
        trace = ReasoningTrace(
            model_type=model_type,
            reasoning_content=reasoning_content,
            reasoning_tokens=reasoning_tokens,
            timestamp=datetime.now(),
            metadata=metadata
        )

        # è®°å½•
        self.reasoning_traces.append(trace)
        self.total_reasoning_tokens += reasoning_tokens

        if self.enable_detailed_logging:
            logger.info(f"ğŸ§  [æ¨ç†å¤„ç†å™¨] æå–åˆ°æ¨ç†å†…å®¹")
            logger.info(f"   - æ¨¡å‹: {model_type.value}")
            logger.info(f"   - é•¿åº¦: {len(reasoning_content)} å­—ç¬¦")
            logger.info(f"   - Tokens: {reasoning_tokens}")

        return trace

    def format_reasoning_display(self, trace: ReasoningTrace) -> str:
        """
        æ ¼å¼åŒ–æ¨ç†è¿‡ç¨‹å±•ç¤º

        Args:
            trace: æ¨ç†è½¨è¿¹

        Returns:
            æ ¼å¼åŒ–çš„å±•ç¤ºæ–‡æœ¬
        """
        lines = []

        # æ ‡é¢˜
        model_name_map = {
            ReasoningModelType.OPENAI_O1: "OpenAI o1",
            ReasoningModelType.DEEPSEEK_R1: "DeepSeek R1",
            ReasoningModelType.CLAUDE_THINKING: "Claude Extended Thinking"
        }

        model_display_name = model_name_map.get(trace.model_type, "Unknown Model")

        lines.append("---")
        lines.append(f"## ğŸ§  {model_display_name} æ¨ç†è¿‡ç¨‹")
        lines.append("")

        # å…ƒä¿¡æ¯
        lines.append(f"**æ¨¡å‹**: {trace.metadata.get('model', 'unknown')}")
        lines.append(f"**æ¨ç† Tokens**: {trace.reasoning_tokens:,}")
        lines.append(f"**æ¨ç†å­—æ•°**: {trace.get_word_count():,}")
        lines.append(f"**æ—¶é—´**: {trace.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")

        # æ¨ç†å†…å®¹
        reasoning_content = trace.reasoning_content

        # æˆªæ–­è¿‡é•¿å†…å®¹
        if len(reasoning_content) > self.reasoning_max_display:
            reasoning_content = reasoning_content[:self.reasoning_max_display]
            truncated = True
        else:
            truncated = False

        lines.append("**æ¨ç†å†…å®¹**:")
        lines.append("```")
        lines.append(reasoning_content)
        lines.append("```")

        if truncated:
            lines.append("")
            lines.append(f"*ï¼ˆæ¨ç†è¿‡ç¨‹å·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹ {len(trace.reasoning_content)} å­—ç¬¦ï¼Œä»…æ˜¾ç¤ºå‰ {self.reasoning_max_display} å­—ç¬¦ï¼‰*")

        lines.append("")

        # ç‰¹æ®Šè¯´æ˜
        if trace.model_type == ReasoningModelType.OPENAI_O1:
            lines.append("ğŸ’¡ **å…³äº OpenAI o1**:")
            lines.append("- o1 ç³»åˆ—æ˜¯ OpenAI çš„æ¨ç†æ¨¡å‹ï¼Œåœ¨å›ç­”å‰è¿›è¡Œæ·±åº¦æ€è€ƒ")
            lines.append("- æ¨ç† tokens å•ç‹¬è®¡è´¹ï¼Œä¸è®¡å…¥è¾“å‡º tokens")
            lines.append("- é€‚åˆå¤æ‚çš„åˆ†æã€æ•°å­¦ã€ç¼–ç¨‹ç­‰ä»»åŠ¡")

        elif trace.model_type == ReasoningModelType.DEEPSEEK_R1:
            lines.append("ğŸ’¡ **å…³äº DeepSeek R1**:")
            lines.append("- R1 æ˜¯ DeepSeek çš„æ¨ç†å¢å¼ºæ¨¡å‹")
            lines.append("- é‡‡ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæå‡å¤æ‚æ¨ç†èƒ½åŠ›")
            lines.append("- å¼€æºæ¨¡å‹ï¼Œæ€§èƒ½æ¥è¿‘ o1-mini")

        elif trace.model_type == ReasoningModelType.CLAUDE_THINKING:
            lines.append("ğŸ’¡ **å…³äº Claude Extended Thinking**:")
            lines.append("- Claude 3 Opus æ”¯æŒæ‰©å±•æ€è€ƒæ¨¡å¼")
            lines.append("- å¯ä»¥å±•ç¤ºè¯¦ç»†çš„æ€è€ƒè¿‡ç¨‹")
            lines.append("- é€šè¿‡ `extended_thinking=True` å‚æ•°å¯ç”¨")

        lines.append("")
        lines.append("---")

        return "\n".join(lines)

    def analyze_reasoning_quality(self, trace: ReasoningTrace) -> Dict[str, Any]:
        """
        åˆ†ææ¨ç†è´¨é‡

        Args:
            trace: æ¨ç†è½¨è¿¹

        Returns:
            è´¨é‡åˆ†ææŒ‡æ ‡
        """
        reasoning = trace.reasoning_content

        # ç®€å•çš„è´¨é‡æŒ‡æ ‡
        metrics = {
            'length': len(reasoning),
            'word_count': trace.get_word_count(),
            'token_count': trace.reasoning_tokens,
            'avg_chars_per_token': len(reasoning) / trace.reasoning_tokens if trace.reasoning_tokens > 0 else 0,
        }

        # æ£€æµ‹æ¨ç†ç»“æ„
        has_steps = any(marker in reasoning for marker in ['æ­¥éª¤', 'Step', '1.', '2.', 'é¦–å…ˆ', 'ç„¶å', 'æœ€å'])
        has_analysis = any(marker in reasoning for marker in ['åˆ†æ', 'Analysis', 'å› ä¸º', 'because', 'æ ¹æ®'])
        has_conclusion = any(marker in reasoning for marker in ['ç»“è®º', 'Conclusion', 'å› æ­¤', 'Therefore', 'ç»¼ä¸Š'])

        metrics['has_structured_thinking'] = has_steps
        metrics['has_analysis'] = has_analysis
        metrics['has_conclusion'] = has_conclusion

        # è´¨é‡è¯„åˆ†ï¼ˆ0-100ï¼‰
        quality_score = 0

        if metrics['word_count'] > 100:
            quality_score += 20  # è¶³å¤Ÿçš„æ¨ç†é•¿åº¦

        if has_steps:
            quality_score += 30  # æœ‰ç»“æ„åŒ–æ€è€ƒ

        if has_analysis:
            quality_score += 30  # æœ‰åˆ†æè¿‡ç¨‹

        if has_conclusion:
            quality_score += 20  # æœ‰ç»“è®º

        metrics['quality_score'] = quality_score

        return metrics

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.reasoning_traces:
            return {
                'total_traces': 0,
                'total_reasoning_tokens': 0,
                'avg_reasoning_tokens': 0,
                'avg_reasoning_length': 0
            }

        total_length = sum(len(t.reasoning_content) for t in self.reasoning_traces)

        # æŒ‰æ¨¡å‹ç±»å‹ç»Ÿè®¡
        model_stats = {}
        for trace in self.reasoning_traces:
            model_type = trace.model_type.value
            if model_type not in model_stats:
                model_stats[model_type] = {
                    'count': 0,
                    'total_tokens': 0
                }

            model_stats[model_type]['count'] += 1
            model_stats[model_type]['total_tokens'] += trace.reasoning_tokens

        return {
            'total_traces': len(self.reasoning_traces),
            'total_reasoning_tokens': self.total_reasoning_tokens,
            'avg_reasoning_tokens': self.total_reasoning_tokens / len(self.reasoning_traces),
            'avg_reasoning_length': total_length / len(self.reasoning_traces),
            'model_distribution': model_stats
        }


# ============================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================

"""
from langchain_openai import ChatOpenAI
from tradingagents.middleware.reasoning_handler import ReasoningHandler, ReasoningModelType

# åˆ›å»ºæ¨ç†å¤„ç†å™¨
reasoning_handler = ReasoningHandler(
    enable_detailed_logging=True,
    reasoning_max_display=1000
)

# ä½¿ç”¨ OpenAI o1 æ¨¡å‹
llm = ChatOpenAI(model="o1-preview", temperature=1)

# æ‰§è¡Œæ¨ç†
messages = [("user", "åˆ†æè´µå·èŒ…å°çš„æŠ•èµ„ä»·å€¼")]
response = llm.invoke(messages)

# æå–æ¨ç†
trace = reasoning_handler.extract_reasoning(response)

if trace:
    # æ ¼å¼åŒ–å±•ç¤º
    display_text = reasoning_handler.format_reasoning_display(trace)
    print(display_text)

    # åˆ†æè´¨é‡
    quality = reasoning_handler.analyze_reasoning_quality(trace)
    print(f"æ¨ç†è´¨é‡è¯„åˆ†: {quality['quality_score']}/100")

# è·å–ç»Ÿè®¡
stats = reasoning_handler.get_stats()
print(f"æ€»æ¨ç†æ¬¡æ•°: {stats['total_traces']}")
print(f"æ€»æ¨ç† tokens: {stats['total_reasoning_tokens']:,}")
print(f"å¹³å‡æ¨ç† tokens: {stats['avg_reasoning_tokens']:.0f}")
"""
